<!doctype html>
<html itemscope itemtype="http://schema.org/Article">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0">

		<title>The Good Life</title>
		<meta name="description" content="What is best in life? How should we live, and behave towards other people? And what are our ethical duties and responsibilities as software engineers, in a world where software controls everything?" />

		<!-- Schema.org markup for Google+ -->
		<meta itemprop="name" content="The Good Life">
		<meta itemprop="description" content="What is best in life? How should we live, and behave towards other people? And what are our ethical duties and responsibilities as software engineers, in a world where software controls everything?">
		<meta itemprop="image" content="https://www.richardwestenra.com/good-life-talk/img/good-life.png">

		<!-- Twitter Card data -->
		<meta name="twitter:card" content="summary_large_image">
		<meta name="twitter:site" content="@richardwestenra">
		<meta name="twitter:title" content="The Good Life">
		<meta name="twitter:description" content="What is best in life? How should we live, and behave towards other people? And what are our ethical duties and responsibilities as software engineers, in a world where software controls everything?">
		<meta name="twitter:creator" content="@richardwestenra">
		<meta name="twitter:image:src" content="https://www.richardwestenra.com/good-life-talk/img/good-life.png">

		<!-- Open Graph data -->
		<meta property="og:title" content="The Good Life" />
		<meta property="og:type" content="article" />
		<meta property="og:url" content="https://www.richardwestenra.com/good-life-talk/" />
		<meta property="og:image" content="https://www.richardwestenra.com/good-life-talk/img/good-life.png" />
		<meta property="og:description" content="What is best in life? How should we live, and behave towards other people? And what are our ethical duties and responsibilities as software engineers, in a world where software controls everything?" />
		<meta property="og:site_name" content="Richard Westenra" />
		<meta property="article:published_time" content="2017-10-26T19:00:00+01:00" />

		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/night.css">
		<link rel="stylesheet" href="css/theme/custom.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/zenburn.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<span class="twitter-handle">@richardwestenra</span>

		<div class="reveal">
			<div class="slides">
<section data-background-color="white">
	<h1 class="title"><span>The</span> <b>Good Life</b></h1>
	<aside class="notes">
		<p>Hi, my name is Richard, and I’m here today to talk to you about what is best in life.</p>
		<p class="meta">(SLOW DOWN)</p>
		<p>I wrote this talk a year ago for a meetup, and Ethics in tech has since become a major topic, and it seems like everyone's talking about it this year. I think it's great that this topic is getting so much attention, but I'm really surprised that that one talk I did at a small meetup last October was so influential?</p>
		<p>Anyway I originally named this talk The Good Life, which I liked as a title because I got to play around with CSS animations for this slide, and then I didn't want to cut it from the slide deck. But actually, it's called <strong>Actually,</strong> it's about ethics in software engineering.</p>
	</aside>
</section>
<section data-background="img/fedora.jpg" data-background-size="contain">
	<h3><b class="actually">Actually,</b> it's about ethics in software engineering</h3>
	<aside class="notes">
		<p>A bit about my background: I started out at university studying mechanical engineering, which I hated. I switched to studying philosophy, which I loved. Now it's ten years later, and my official job title is front end engineer. Go figure.</p>
		<p>So I'm a front-end engineer these days, but I usually refer to myself as a developer. I've always felt a bit uncomfortable with the title of engineer, though I couldn't really put my finger on why. Maybe it felt a bit like I was claiming some sort of prestige that I hadn't earned?</p>
		<p>I didn't complete my engineering degree or study computer science: I'm mostly self-taught. I like to think I’m pretty good at my job, but I still feel like I'm hacking stuff together a lot of the time.</p>
	</aside>
</section>
<section data-background="img/webmaster.jpg" data-background-size="contain">
	<aside class="notes">
		<p>Back in the day, many of us who build websites used to call ourselves webmasters (and webmistresses). Which I still think is an amazing job title, btw.</p>
		<p class="meta">(SLOW DOWN)</p>
</section>
<section data-background="img/literal-webmaster.jpg" data-background-size="contain">
	<aside class="notes">
		<p>I'm kinda sad that I didn't start my career quite early enough to have a webmaster role in the job history section of my CV. It's a bit like Social Justice Warrior, like, people use it as an insult but frankly I think it sounds badass.</p>
		<p>Anyway so we were webmasters and webmistresses...</p>
</section>
<section data-background="img/webmistress.jpg" data-background-size="contain">
	<aside class="notes">
		<p>And then that title fell out of fashion and we became web developers, and now many of us have the very professional-sounding title of front-end engineers.</p>
		<p>I don’t think that's a bad thing, or that we should stop calling ourselves engineers - call yourself whatever you want, you can be a techno-viking or an agile pirate-ninja riding a full-stack glitter-unicorn if it makes you feel better.</p>
	</aside>
</section>
<section data-background="img/spacejam.png" data-background-size="cover">
	<h3 class="hashtag">#ObligatorySpaceJam</h3>
	<aside class="notes">
		<p>But we used to be working on dinky little websites that were kind of thrown together, and now we're working on massive, professional operations that form the financial backbone of multi-billion dollar corporations. We've come a long, long way from being webmasters and webmistresses.</p>
		<p>But a lot of us are self-taught and didn't go through any standardised training process. And don't get me wrong, this is great, it helps ease barriers to entry. But as the industry gets more professional, it might be worth thinking about other trappings of professionalism. If we're going to call ourselves engineers, then there are ethical duties and codes of responsibility that go along with that title.</p>
	</aside>
</section>
<section data-background="img/forth.jpg" data-background-size="cover" data-background-position="right 35%">
	<aside class="notes">
		<p>A hundred years ago, civil engineering was in a similar situation to how the tech industry is now. As the industrial revolution receded behind them, engineers found new ways to use all the fancy new technologies they had developed. They grew more sophisticated in their approach, and their projects ballooned in scale and complexity.</p>
		<p>But as these projects became more ambitious, there was an accompanying problem: A rise in major engineering disasters. The turn of the twentieth century saw a wave of epic structural failures, including some massive bridge collapses,</p>
	</aside>
</section>
	<section data-background="img/molasses.jpg" data-background-size="contain">
	<h4 class="hashtag">#StickySituation</h4>
	<h4 class="hashtag">#TooSoon</h4>
	<aside class="notes">
		<p>&hellip;and also the Great Boston Molasses Flood, which you can see here.</p>
		<p>Which, if I had to name my favourite disaster of all time, this would have to be it, just for the mental image of a tsunami of liquid sugar 50 feet high, travelling 35 miles an hour, consuming everything in its path. It's terrifying, but also, kinda delicious?</p>
		<p>Anyway these disasters had a profound effect on the way the public saw engineering, and forced engineers to confront their shortcomings.</p>
		<p>As a result, they began to regulate themselves more intensely, and established standardised industry codes of ethics.</p>
	</aside>
</section>
<section data-background="img/conan.gif" data-background-size="cover">
	<aside class="notes">
		<p>So, what is Ethics?</p>
		<p>Ethics is a branch of philosophy devoted to answering questions about what is best in life. Questions like these:</p>
	</aside>
</section>
<section data-background="img/conan.gif" data-background-size="cover">
	<dl class="ethics">
		<dt>“What is best?”</dt>
		<dd class="fragment">Both spaces AND tabs, on alternating lines</dd>
		<dt>“What is the good life?”</dt>
		<dd class="fragment">When the client is banned from feature requests</dd>
		<dt>“How should I live?”</dt>
		<!-- <dd class="fragment">Only support one browser</dd> -->
		<dd class="fragment">Outsource your job to China and spend all day on Reddit</dd>
		<dt>“How should I behave towards other people?”</dt>
		<dd class="fragment">Interrupt them when they have their headphones on</dd>
		<dt>“What is the purpose of life?”</dt>
		<dd class="fragment">Replacing everything with JavaScript</dd>
	</dl>
	<aside class="notes">
		<p>And I know what you're thinking, I can see the cogs in your software developer minds turning over. You're thinking "That's EASY!"</p>
		<p class="meta">(DOWN x 4)</p>
		<p>...You're all monsters.</p>
		<p>Though I'm with you on that client thing, I'd like to go one step further and ban all clients, but that's a different topic for another day.</p>
	</aside>
</section>
<section data-background="img/chidi.jpg" data-background-size="cover">
	<aside class="notes">
		<p>Philosophers like to do things called "thought experiments", which are like real experiments but even better, because you never need to get out of your armchair.</p>
		<p>One of the most famous is the trolley problem. I'm sure many of you are already familiar with it, but for those who aren't, here's the deal.</p>
	</aside>
</section>
<section data-background="img/trolley-problem.jpg" data-background-color="white" data-background-size="contain">
	<aside class="notes">
		<p>There is a runaway trolley barreling down the railway tracks. Ahead, on the tracks, there are five people tied up and unable to move. The trolley is headed straight for them. You are standing some distance off in the train yard, next to a lever. If you pull this lever, the trolley will switch to a different set of tracks. However, you notice that there is one person on the side track. You have two options:</p>
		<p>Do nothing, and the trolley kills the five people on the main track.</p>
		<p>Pull the lever, diverting the trolley onto the side track where it will kill one person.</p>
		<p>Which is the most ethical choice? Quick show of hands, no wrong answers.</p>
	</aside>
</section>
<section data-background="img/fat-man.jpg" data-background-color="white" data-background-size="contain">
	<aside class="notes">
		<p>Now, imagine instead of a switch, you’re standing on a bridge over the tracks, next to an extremely large man. The trolley is coming, and the only way you can stop it is to push the large man onto the tracks. He’s the only one big enough to slow down the trolley.</p>
		<p>He’s looking you right in the eyes, he can see what you’re thinking, and he’s terrified, begging you not to do it. What do you do?</p>
	</aside>
</section>
<section>
	<p><div class="emoji">🛠</div> 9 out of 10 people would<br> throw the switch,</p>
	<p><div class="emoji emoji--gap">🙅‍♂️</div> but only 1 in 10 people would<br> push the large man.</p>
	<aside class="notes">
		<p>The trolley problem has been the subject of many surveys, which tend to find that approximately 9 out of 10 respondents would throw the switch to kill the one and save the five.</p>
		<p>However in the large man scenario, the situation reverses, and only 1 in 10 people would push him onto the tracks.</p>
		<p>Incidentally, a 2009 survey of professional philosophers found that only 68% of them would throw the switch, 8% would not switch, and the remaining 24% had another view or could not answer. So if you’re ever tied to a train track by a cartoon villain, you’d better hope that the person by the switch isn’t a moral philosopher.</p>
	</aside>
</section>
<section data-background="img/brain.gif" data-background-size="cover">
	<aside class="notes">
		<p>So why the difference in the two outcomes?</p>
		<p>One theory is that it's because two different parts of your brain are fighting with each other.</p>
		<p>Some researchers looked at people’s brains using fMRI machines, and demonstrated that "personal" dilemmas (like pushing a man off a footbridge) engage brain regions associated with emotion, whereas "impersonal" dilemmas (like diverting the trolley by flipping a switch) engaged regions associated with controlled reasoning.</p>
		<p>And these different brain processes essentially compete with each other whenever you have to make a tough moral decision.</p>
	</aside>
</section>
<section data-background="img/mvr.jpg" data-background-color="white" data-background-size="contain">
	<aside class="notes">
		<p>Basically, inside your brain, you’ve got a monkey and a robot fighting over the controls. Every time you have to make a moral decision, they duke it out.</p>
		<p>The monkey understands something simple like pushing someone off a bridge, and it's horrified. But it doesn't understand something complex like a mechanical switch, so in that situation, the gut response is reduced, and we're able to throw the lever without feeling such a crushing sense of moral horror.</p>
		<p>Now some people have a stronger monkey, and some people have a stronger robot, and that's great, because both are useful in different situations.</p>
		<p>But this is tricky for we programmers, because we work on complex problems, which might make it trickier for our monkey brains to trigger some kinds of moral responses.</p>
	</aside>
</section>
<section data-background="img/self-driving-cars.jpg" data-background-size="cover">
	<aside class="notes">
		<p>By the way, if you think it's hard for programmers to experience the full range of ethical response, then spare a thought for autonomous vehicles.</p>
		<p>Self-driving cars don’t have meat brains, and you can’t make a perfectly ethical algorithm: You can only make it as good as the humans who programmed it, and we can’t even agree on whether to use tabs or spaces.</p>
	</aside>
</section>
<!-- <section data-background="img/fibonacci.png" data-background-size="contain" data-background-color="white">
	<aside class="notes">
		Incidentally, the correct answer is increasing the spacing for each successive indent according to the Fibbonaci sequence, but I digress.
	</aside>
</section> -->
<section data-background="img/lidar.jpg" data-background-size="cover">
	<aside class="notes">
		<p>There are also some really tricky problems here that self-driving cars will face. Like, we'd prefer a self-driving car to swerve into a pile of trash rather than hit someone, just like a person might. But computers can make these kinds of decisions much quicker than we can, so if we decide in advance what we want them to do, they'll follow our instructions.</p>
		<p>So, we'd probably want to program our car to hit a single adult rather than bus-load of school children, right? But what if the adult is a Nobel-prize winning cancer researcher? What if the adult is driving the car? Would we want the car to sacrifice its driver? And would you choose to buy a self-driving car that's designed to sacrifice your life to save others?</p>
	</aside>
</section>
<section data-background="img/moralmachine.png" data-background-size="cover" data-background-position="center top">
	<p class="hashtag" style="margin: 0">moralmachine.mit.edu</p>
	<aside class="notes">
		<p>Some researchers at MIT came up with a nice solution for this: They built an app to mine data on people’s answers to different trolley problems, so they can use it to help them decide how autonomous cars should behave in different scenarios.</p>
		<p>The website is called Moral Machine, and you can go there right now and start judging scenarios, like this one, where you have to choose between a male athlete driving a car, and a jaywalking baby. On the one hand, the baby probably doesn't know not to cross on a red signal, but on the other hand, it might grow up to be Hitler, you know? So it's a tough call.</p>
		<!-- <p>Anyway, Moral Machine is cool, but it doesn't help us, because we can't outsource all our ethical decision-making to the internet. We're just individuals working on our laptops, and we have these ridiculous meat brains, and we have to make our own decisions about things like whether to kill baby Hitler.</p>
		<p>So of course, we sometimes make the wrong call.</p> -->
	</aside>
</section>
</section>
<section data-background="img/moralmachine2.jpg" data-background-size="contain" data-background-color="white">
	<aside class="notes">
		<p>And it turns out that we're collectively pretty bad at these sorts of moral calculations. MIT released some of their results a couple of weeks ago, and we can learn two things from their data:</p>
		<p>1. People would prefer that criminals die in car accidents rather than dogs. But they value the lives of criminals over cats.</p>
	</aside>
</section>
<section>
	<div class="criminals"></div>
	<h4>"Dogs > Criminals > Cats"<br>- Science</h4>
	<aside class="notes">
		<p>1. People would prefer that criminals die in car accidents rather than dogs. But they value the lives of criminals over cats.</p>
	</aside>
</section>
<section data-background="img/doctorwho.jpg" data-background-size="cover">
	<div class="doctors"></div>
	<h4>"Male Doctor > Female Doctor"<br>- Science</h4>
	<aside class="notes">
		<p>2. BBC viewers still narrowly prefer the previous incarnations of Doctor Who.</p>
		<p>Though another interpretation of these results is that in the future, we'll all be walking around wearing a stethoscope and lab coat, so that self-driving cars don't murder us.</p>
		<p>Anyway, Moral Machine is a cool idea, but it doesn't help us, because we can't outsource all our ethical decision-making to the internet. We're just individuals working on our laptops, and we have these ridiculous meat brains, and we have to make our own decisions about things like whether to kill baby Hitler.</p>
		<p>So of course, we sometimes make the wrong call.</p>
	</aside>
</section>
<section data-background="img/volkswagen.jpg" data-background-size="cover">
	<section class="vw">
		<p>Volkswagen modified 11 million cars with software that would detect when they were being tested, and change performance to pass emissions tests.</p>
		<p class="fragment">As a result, these engines emitted nitrogen oxide pollutants up to 40 times above what is allowed by US emissions standards.</p>
		<aside class="notes">
			<p>Let's shift gears a little, and consider the Volkswagen emissions scandal. You may recall that VW added special software to millions of diesel cars, that would detect when their exhaust fumes were being checked by emissions regulators, and change performance to pass these tests.</p>
			<p class="meta">(DOWN)</p>
			<p>As a result, they managed to completely bypass emissions standards in the US, EU and elsewhere, for a period of about five years. Their workaround allowed them to emit up to 40 times more nitrogen oxide than what US emissions standards allow.</p>
			<p class="meta">(DOWN)</p>
		</aside>
	</section>
	<section class="vw">
		<p>Air pollution causes 40,000 early deaths per year in the UK alone.</p>
		<aside class="notes">
			<p>By some estimates, air pollution causes around 40,000 early deaths per year in the UK alone.</p>
			<p>It's pretty safe to assume that Volkswagen's technical hack is likely to result in several thousand premature deaths, plus thousands more cases of asthma and other lung disease.</p>
			<p class="meta">(DOWN)</p>
		</aside>
	</section>
	<section class="vw">
		<p>In August 2017, a former Volkswagen engineer was sentenced to three years in prison and fined $200,000 for his part in the scandal. Seven others are still facing charges.</p>
		<aside class="notes">
			<p>And as someone who recently started experiencing asthma symptoms for the first time since I was a child - probably because of London's air pollution - I take this pretty personally.</p>
			<p>So when I heard last year that one of the engineers at VW was imprisoned for his role in the scandal, I thought "GOOD".</p>
		</aside>
	</section>
</section>
<section data-background="img/grumpy.jpg" data-background-size="cover">
	<h1 class="grumpy">GOOD</h1>
	<aside class="notes">
		<p>But on the other hand, I've got to give credit where it's due: VW's "defeat device" is a brilliant technical hack. It's ingenious. I can imagine that the engineers who created it must have felt pretty proud of themselves at the time.</p>
	</aside>
</section>
<section data-background="img/baddies.gif" data-background-size="contain">
	<aside class="notes">
		<p>But at the same time, you also wonder why nobody spoke up at one of their internal meetings to say hey pals, do you think that maybe we're assholes?</p>
		<p>How did they get it so wrong? Are they just inherently bad people?</p>
	</aside>
</section>
<section data-background-color="#E5D6C7">
	<img src="img/bored-monkey.png" class="plain" />
	<aside class="notes">
		<p>Maybe it's because the monkey part of their brain was completely unable to deal with the complexity of the problem. You've got cars, and software hacks, and air pollution, and then decades later some people you don't know might die... It's all a bit much for the poor monkey to handle.</p>
	</aside>
</section>
<section data-background="img/forgetful.png" data-background-color="white" data-background-size="contain">
	<aside class="notes">
		<p>We established earlier that ethical reasoning involves an internal struggle for control. And a weird thing about humans is that we can sometimes actually ‘forget’ to act ethically, when we’re so focussed on achieving a goal that we forget to think about consequences of our actions.</p>
		<p>Or else we justify it to ourselves in ways that don’t stand up to scrutiny, but never stop to properly reflect.</p>
		<p>I’m sure we’ve all done this at some point, I definitely have, and it’s led to some of my biggest screwups.</p>
	</aside>
</section>
<section data-background="img/move-fast-and-break-things.jpg" data-background-size="contain">
	<aside class="notes">
		<p>When you're just looking at a wall of code, it's very easy to forget about the humans who will be affected by your decisions IRL. And unlike civil engineering, it's usually easy to fix mistakes: Just roll out a patch or an update.</p>
		<p>In tech, we like to move fast and break things. But we don’t want to move fast into oncoming traffic and break people.</p>
	</aside>
</section>
<section>
	<section data-background="img/scandals.png" data-background-size="contain">
		<aside class="notes">
			<p>I think the monkey brain is a factor in many of the biggest ethical lapses in tech today. Whether it's Facebook enabling fake news and Equifax with their criminally sloppy security, or JavaScript devs being too lazy to bother making their websites accessible for disabled people and keyboard users.</p>
			<p class="meta">(DOWN AND BACK)</p>
			<p>I want to believe that the people making these decisions are doing so because they're not thinking hard enough about the consequences and the people affected by their actions.</p>
		</aside>
	</section>
	<section data-background="img/point.jpg" data-background-size="contain" data-background="white">
		<aside class="notes">
			<p>I want to believe that the people making these decisions are doing so because they're not thinking hard enough about the consequences and the people affected by their actions.</p>
		</aside>
	</section>
</section>
<section data-background="img/vonbraun.jpg" data-background-size="contain">
	<blockquote class="vonbraun">&lsquo;&ldquo;Once the rockets are up, who cares where they come down? That's not my department,&rdquo; says Wernher von Braun.&rsquo; &mdash; Tom Lehrer</blockquote>
	<aside class="notes">
		<p>However there are also those who say "Hey I don't know about all that ethics stuff, I'm just an engineer! It's not my responsibility."</p>
		<p>Like Mr Von Braun here, who knew he couldn't be in the forefront of rocket research in Nazi Germany if he didn't go along – and he didn't care what crimes to turn a blind eye to as long as he was allowed to play with his rockets.</p>
	</aside>
	<!-- http://www.sciencemag.org/news/2018/02/artificial-intelligence-could-identify-gang-crimes-and-ignite-ethical-firestorm -->
</section>
<section>
	<h3>Nobody is exempt from having to behave ethically</h3>
	<aside class="notes">
		<p>So to be clear: Nobody is exempt from having to behave ethically. Scientists and engineers aren't a special group that get to be amoral, and don't have to think about this stuff. Ethics contaminates everything.</p>
		<p>Whether you're building rockets, or designing algorithms to help police identify gang members, you have a duty to consider how they might be used.</p>
	</aside>
</section>
<section data-background="img/reminders.png" data-background-size="cover" data-background-color="#F5F5F5">
	<aside class="notes">
		<p>With so many examples of ethically-compromised decision-making in tech, it's easy to get pessimistic.</p>
		<p>There's some good news, though: If it's easy for people to act unethically when they don't think about it, then the flip side to this, is that it follows that people tend to behave more ethically when you remind them to.</p>
		<p>And it can happen even when you do it in subtle ways.</p>
	</aside>
</section>
<section>
	<img src="img/eyes.png" class="plain" alt="Eyes emoji">
	<aside class="notes">
		<p>For instance, some researchers in Newcastle found that just hanging up posters of staring human eyes in a cafetaria was enough to significantly change people's behaviour, and it made people twice as likely to clean up after themselves.</p>
		<p>If just a poster of eyes can achieve that much, then imagine what else we could accomplish with a few well-placed reminders.</p>
	</aside>
</section>
<section>
	<h3>Use reminders <br>to help developers make <br>ethical decisions</h3>
	<aside class="notes">
		<p>We want to establish an organisational culture where people tend to act morally, and where there are lots of positive examples for us to emulate. And I think that reminders are a powerful tool to help us achieve this.</p>
	</aside>
</section>
<section>
	<h2>Establish <br>codes of ethics</h2>
	<aside class="notes">
		<p>I mentioned before that many engineering industry bodies introduced formal codes of ethics in the early twentieth century.</p>
		<p>These came along with more legal regulations and barriers to entry, which I don't think is good for our industry. But ethical codes are a great idea!</p>
		<p>These are a very good way to remind people to act ethically. Because basically, when you tell people “hey don’t be a dick”, they will be less likely to be a dick.</p>
		<p>We already do this with codes of conduct at conferences and other events (including EMF Camp), and open-source Github repositories.</p>
	</aside>
</section>
<section>
	<blockquote class="code-of-ethics">
		<h3>Code of Ethics of the American Society of Civil Engineers</h3>
		<ol>
			<li>Engineers shall hold paramount the safety, health and welfare of the public and shall strive to comply with the principles of sustainable development in the performance of their professional duties.</li>
			<li>Engineers shall perform services only in areas of their competence.</li>
			<li>Engineers shall issue public statements only in an objective and truthful manner.</li>
			<li>Engineers shall act in professional matters for each employer or client as faithful agents or trustees, and shall avoid conflicts of interest.</li>
			<li>Engineers shall build their professional reputation on the merit of their services and shall not compete unfairly with others.</li>
			<li>Engineers shall act in such a manner as to uphold and enhance the honor, integrity, and dignity of the engineering profession and shall act with zero-tolerance for bribery, fraud, and corruption.</li>
			<li>Engineers shall continue their professional development throughout their careers, and shall provide opportunities for the professional development of those engineers under their supervision.</li>
		</ol>
	</blockquote>
	<aside class="notes">
		<p>We can do this at our organisations too. They don't have to be complicated, in fact, the simpler the better. The most important thing is to set appropriate expectations for ethical behaviour.</p>
	</aside>
</section>
<section>
	<h3>Computing codes of ethics:</h3>
	<ul>
		<li>Association for Computer Machinery</li>
		<!-- https://www.acm.org/code-of-ethics -->
		<li>Australian Computer Society</li>
		<li>British Computer Society</li>
		<li>Computer Ethics Institute</li>
	</ul>
	<p><br><br><small>Source: <a href="https://www.youtube.com/watch?v=GxKLgs8Vgsc">Patricia Realini - JSConf EU 2018</a></small></p>
	<aside class="notes">
		<p>There are loads of other codes of ethics around that you can use as inspiration. Read over some of the different codes, discuss them with your colleagues, and think about what sort of ethical principles you'd choose for your own work, your team and your company. You can use an existing code of ethics, or borrow different aspects and make your own.</p>
	</aside>
</section>
<section>
	<h2>Communicate expectations</h2>
	<aside class="notes">
		<p>Once you've chosen an ethical code, communicate it within your team. How you communicate it is up to you.</p>
		<p>For example you could include it in your onboarding for new starters, add ethical checks to checklists and documentation for new projects, or run internal publicity campaigns. The important thing is that it becomes part of your team or company culture.</p>
		<p>This act of communicating expectations is important for empowering team members to speak up if they're uncomfortable, before it's too late.</p>
	</aside>
</section>
<section>
	<h2>Empower developers<br> to feel able to say no</h2>
	<aside class="notes">
		<p>A few years ago, I was working for a consultancy, who assigned me to a website project for a client I didn't approve of. But I got so invested in solving the technical aspects of the project, that I didn't stop to think about whether I was morally okay with working for this client, until I was already deeply invested. I moaned about the client to some colleagues, and they told me, if you didn't want to work for this client, that's fine, but you should have said something at the start of this project.</p>
		<p>It made me realise that it was okay to say no to client projects, but also that the appropriate time to do that is before you start work. The later you leave it, the harder it is to say no.</p>
		<p>The next time a dodgy client came along, I felt more comfortable expressing my concerns up-front, and we ended up turning down the client.</p>
	</aside>
</section>
<section>
	<section data-background="img/you-wouldnt.png" data-background-color="black" data-background-size="contain">
		<aside class="notes">
			<p>If we establish policies ahead of time to say that it's okay to speak up if you're uncomfortable, then we can avoid these kinds of situations.</p>
			<p>I tend to think of this as being like encouraging developers to submit bug reports and point out problems in your applications or processes. If everyone feels empowered to speak up, then you're all better off.</p>
			<p>DOWN?</p>
		</aside>
	</section>
	<section data-background="img/Challenger_explosion.jpg" data-background-size="cover">
		<h2 style="text-shadow: -2px 4px 0 rgba(0,0,0,0.3), 0 0 40px rgba(0,0,0,0.5)">Whistleblowing</h2>
		<aside class="notes">
			<p>On a related note note: If you speak up about ethically dubious practices at your workplace and your employer doesn't listen, you may have a duty to report it to the authorities or otherwise make it public!</p>
			<p>A basic dilemma in engineering ethics is that an engineer has a duty to their client or employer, but an even greater duty to report a possible risk to others from a client or employer failing to follow the engineer's directions.</p>
			<p>A classic example of this is the Challenger Space Shuttle Disaster. NASA engineers raised warnings about faulty O-rings in the boosters, and the dangers posed by the low temperatures on the morning of the launch, but managers disregarded these warnings, and failed to adequately report these technical concerns to their superiors.</p>
			<p>It was later argued that in these circumstances, the engineers had had a duty to circumvent their managers and shout about the dangers until they were heard.</p>
		</aside>
	</section>
</section>
<section>
	<h2>Ethics checks</h2>
	<aside class="notes">
		<p>I mentioned building ethics checks into processes, as a regular reminder to encourage ethical thinking as early as possible.</p>
		<p>A friend of mine who works as a psychotherapist tells me that their training includes ethics-checks as a core part of their process.</p>
		<p>So whenever they are trying to make a tough decision, they have these questions they can use, which are designed to trigger different types of emotional responses.</p>
	</aside>
</section>
<section>
	<p><div class="emoji">🐒</div> Would you be happy for everyone to know<br> the decision you’ve made?</p>
	<p><div class="emoji emoji--gap">🤖</div> Do you think the consequences are acceptable?</p>
	<p><div class="emoji emoji--gap">🤔</div> Would you recommend the same course<br> of action to others?</p>
	<aside class="notes">
		<p>So you've got the first one here, which I think is quite a monkey brain question, it's a good one for triggering emotional reactions like shame.</p>
		<p>So for example, if you're considerig being lazy about making your website accessible, imagine there's a disabled person sitting next to you, and think about whether you'd feel comfortable explaining your code choices to them.</p>
		<p>The second one seems designed to trigger more of a consequentialist response. It's maybe a bit of a rationalist, robot-brained approach?</p>
		<p>The last one reminds of Immanuel Kant's categorical imperative, that you should only do something if you'd be okay with it being a universal law. So that's cool too, if Kant's your thing.</p>
		<p>I think these are a great start, but feel free to build off them or tailor them to your own work.</p>
	</aside>
</section>
<section>
	<h3>Encourage developers<br> to meet users</h3>
	<aside class="notes">
		<p>Finally, we can help engineers to develop more empathy for their users, by encouraging them to meet them in person. A great way to do this is to get devs to sit in on user-testing sessions.</p>
		<p>A nice additional benefit of this is that empathy for your users helps you design better user-centred solutions, too. Win-win.</p>
	</aside>
</section>
<section>
	<h2>This is just the beginning</h2>
	<aside class="notes">
		<p>These ideas are just a start, they won't fix everything.</p>
		<p>For example, it won't put a stop to the fact that a small handful of megacorporations own our digital lives and strip-mine them for profit.</p>
		<p>But you know, one step at a time.</p>
	</aside>
</section>
<section>
	<h1>Thanks!</h1>
	<p style="margin: 1em 0 0.8em">Don't forget to like and subscribe 👍</p>
	<a class="social" style="left:-1.85em" href="http://twitter.com/richardwestenra"> <i class="icon-twitter"></i> twitter.com/<b>richardwestenra</b></a>
	<a class="social" style="left:-1.85em" href="http://github.com/richardwestenra"> <i class="icon-github"></i> github.com/<b>richardwestenra</b></a>
	<a class="social" style="left:0em" href="mailto:richard@richardwestenra.com"> <i class="icon-email"></i> richard@<b>richardwestenra</b>.com</a>
	<a class="social" style="left:2.17em" href="http://richardwestenra.com"> <i class="icon-home"></i> <b>richardwestenra</b>.com</a>
	<!-- <a class="social" style="left:-2.8em" href="http://linkedin.com/in/richardwestenra"> <i class="icon-linkedin"></i> linkedin.com/in/<b>richardwestenra</b></a> -->
	<!-- <a class="social" style="left:-2.5em" href="http://facebook.com/richardwestenra"> <i class="icon-facebook"></i> facebook.com/<b>richardwestenra</b></a> -->
	<aside class="notes">
		<p>By the way if you have any questions or suggestions, please come chat to me afterwards, or get in touch online using any of my imaginative usernames.</p>
	</aside>
</section>
<!-- <section>
	<img src="img/socrates.jpg" class="plain" alt="Socrates" width="550px">
	<h2>Questions?</h2>
	<aside class="notes">
		<p>I'm happy to try to answer any questions you might have!</p>
		<p>However you might find that, like Socrates here, I only know that I know nothing.</p>
	</aside>
</section> -->
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				history: true,
				showNotes: window.location.search.match(/showNotes/gi),
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
